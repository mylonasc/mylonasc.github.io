---
layout: post
title: Tensor Sketching and Scalable Machine learning
subtitle: 
bigimg: /img/bannerAcLoc.png
tags: [Deep Neural Networks, Dropout, Matrix Sketching, Canonical Decomposition]
---
# Motivation/context
Tensors are arrays with more than two indices (multi-indexed arrays). They naturally occur in computational physics when
 considering basis functions for different quantities or dimensions, and their products. For example the coefficients 
of a Fourier series approximation for a scalar quantity in 1D can be represented as a vector (order-1 tensor):

$$
F(x_1) \sim \sum_{k} a_i e^{i i x_1} = \begin{bmatrix}a_1 \\ a_2 \\ \vdots \end{bmatrix} * \begin{bmatrix}e^{1} \\ e^{2} \\ \vdots \end{bmatrix}
$$

Correspondingly, the coefficients of a Fourier transform for a scalar quantity in 2D can be represented as a matrix (order-2 tensor) and a Fourier transform 
for a scalar quantity in three dimensions as an order 3 tensor. In order to establish notation, here is the 3-dimensional fourier transform in multi-indexed notation:
$$
F(x_1,x_2,x_3) = \sum_{k,l,m} C_{klm} e^{k x_1 *  i} e^{l x_2  * i} e^{m x_3 * i }
$$

The more dimensions a tensor has:
* the more storage requirements it has
* the computational requirements when using the tensor increase


All computational scientists at some point need to work with quadrature in order to integrate efficiently. Multi-dimensional quadrature is perhaps the most common application of tensors. 
Due to the aforementioned issues (high storage and processing needs) it is often necessary to work on an approximation of such quantities. One common approximation of these tensors is the so-called low-rank canonical decomposition (here is a [talk](https://www.youtube.com/watch?v=L8uT6hgMt00) by T. Kolda on tensor decompositions that gives a nice introduction and the interesting history behind the naming of this decomposition). I'll be refering to it as canonical decomposition in the following.

### Establishing terminology and notation
An approximation (from now on refered to as rank-one approximation) of $C_{klm} $ may be obtained as such:
$\require{\wasysym}$

$$
C_{klm} = \mathbf{a} \ocircle \mathbf{b} \ocircle \mathbf{c} 
$$

where $$ [a_1,a_2,a_3] \ocircle [b_1,b_2,b_3] = [a_1*b_1, a_1*b_2, a_1*b_3 , a_2*b_1, a_2*b_2, a_2 * b_3, a_3*b_1, a_3*b_2, a_3 * b_3 ] $$ with $$ \mathbf{a} = \{ a_1, a_2, \cdots \} $$. 
Notice we had 3 coefficients and the **unfolded** version of the same rank-one approximation is dimension 9.


My real introduction to the canonical decompositions was when I worked on 
[UQLAB](https://www.uqlab.com/lra-user-manual), where canonical 
tensor decompositions are used in order to avoid performing regression on the extremely large regression basis sets that come up in the context of tensor regression with orthogonal polynomials (Polynomial Chaos Expansions). In the UQLab LRA library, my former colleague K. Konakli and I, implemented optimization procedures relying on alternating least squares to compute canonical decompositions of the large regression basis sets without ever actually having to *unfold* the tensor of regression coefficients. However, the alternating least squares technique is ridden with problems:

* There is a strong reliance on the 

## What is CP-Grad and why should you care about it?
As mentioned, tensors are sometimes really big. This has over-arching implications. The one CP-Grad tries to solve, is being able to perform optimization with mini-batches of the data, but with batches coming from potentially different *fibers* of the tensor. 


![harmonics](/img/guitarharm.jpg)

